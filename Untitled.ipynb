{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a518f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self,n_arms, probabilities): \n",
    "        self.n_arms= n_arms\n",
    "        self.probabilities= probabilities\n",
    "        \n",
    "    def round(self,pulled_arm): #function the model the interaction with the learner\n",
    "        reward=np.random.binomial(1,self.probabilities[pulled_arm])\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdea7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner:\n",
    "    def __init__(self,n_arms):\n",
    "        self.n_arms= n_arms\n",
    "        self.t=0\n",
    "        self.rewards_per_arm = x = [[]for i in range(n_arms)]\n",
    "        self.collected_rewards=np.array([])\n",
    "        \n",
    "    def update_observations(self,pulled_arm,reward):\n",
    "        self.rewards_per_arm[pulled_arm].append(reward) \n",
    "        self.collected_rewards= np.append(self.collected_rewards,reward)\n",
    "        \n",
    "        #line 18 and 19 do the same thing but respectively on pyhton list an np array\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344abd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS_Learner(Learner): \n",
    "    def __init__ (self,n_arms): #extend the class Learner so we pass the same variables\n",
    "        super().__init__(n_arms)\n",
    "        self.beta_parameters= np.ones((n_arms,2))\n",
    "        \n",
    "    def pull_arm(self):\n",
    "        idx= np.argmax(np.random.beta(self.beta_parameters[:,0],self.beta_parameters[:,1])) #choice of the pulled arm\n",
    "        return idx\n",
    "    \n",
    "    def update(self,pulled_arm,reward):\n",
    "        self.t+=1\n",
    "        self.update_observations(pulled_arm,reward)\n",
    "        self.beta_parameters[pulled_arm,0]= self.beta_parameters[pulled_arm,0]+reward\n",
    "        self.beta_parameters[pulled_arm,1]= self.beta_parameters[pulled_arm,1]+1.0-reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c21daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliThompsonSocket( PowerSocket ):\n",
    "    def __init__( self, q ):             \n",
    "                \n",
    "        self.α = 1  # the number of times this socket returned a charge        \n",
    "        self.β = 1  # the number of times no charge was returned\n",
    "        \n",
    "        # pass the true reward value to the base PowerSocket             \n",
    "        super().__init__(q)          \n",
    "    \n",
    "    def charge(self):        \n",
    "        \"\"\" return some charge with the socket's predefined probability \"\"\"\n",
    "        return np.random.random() < self.q\n",
    "                    \n",
    "    def update(self,R):\n",
    "        \"\"\" increase the number of times this socket has been used and \n",
    "            update the counts of the number of times the socket has and \n",
    "            has not returned a charge (alpha and beta)\"\"\"\n",
    "        self.n += 1    \n",
    "        self.α += R\n",
    "        self.β += (1-R)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\" return a value sampled from the beta distribution \"\"\"\n",
    "        return np.random.beta(self.α,self.β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04558ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Context.Customer_Definition import *\n",
    "from Price_Optimization.Learner_Price import *\n",
    "\n",
    "np.random.seed(29344)\n",
    "\n",
    "class TSLearner(Learner):\n",
    "    def __init__(self, n_arms, bird):\n",
    "        super().__init__(n_arms, bird)\n",
    "        self.beta_parameteres = np.ones([n_arms, 2])\n",
    "        self.fixed_cost = parameters[\"FIXED_CONSTANTS\"][\"FIXED_COST\"]\n",
    "        self.poisson_lambda = 0\n",
    "        self.past_arms = []\n",
    "        self.past_purchase = []\n",
    "        self.collected_reward = []\n",
    "        self.n_of_purchase = 0\n",
    "        self.n_plays = np.zeros(n_arms)\n",
    "        self.prob = 0\n",
    "        self.value_mean = np.zeros(10)\n",
    "        self.time_pulled = np.zeros(10)\n",
    "        self.bird = bird\n",
    "\n",
    "    def value(self, pulled_arm_price, estimated_conversion_rate):\n",
    "        return estimated_conversion_rate * (prices[pulled_arm_price] - self.fixed_cost) * (1 + self.poisson_lambda)\n",
    "\n",
    "    def pull_arm(self, t):\n",
    "        upper_bound = np.random.beta(self.beta_parameteres[:, 0], self.beta_parameteres[:, 1])\n",
    "        time = t - self.bird\n",
    "        obj = np.zeros(self.n_arms)\n",
    "        for i in range(self.n_arms):\n",
    "            obj[i] = self.value(i, upper_bound[i])\n",
    "        pulled_arm = np.random.choice(np.where(obj == obj.max())[0])\n",
    "        self.past_arms.append(pulled_arm) #CAMBIATO: messo fuori dall'else\n",
    "        return pulled_arm\n",
    "\n",
    "    def update(self, pulled_arm_price, n_purchases, n_clicks, daily_reward, n_returns, prob, t):\n",
    "        time = t - self.bird\n",
    "        self.collected_reward.append(n_purchases)\n",
    "        self.collected_objective_function.append(daily_reward)\n",
    "        #print(\"daily_obj: \", daily_reward)\n",
    "        if time > 29:\n",
    "            den = max(1, self.n_of_purchase + self.collected_reward[time-30])\n",
    "            self.poisson_lambda = (self.poisson_lambda * self.n_of_purchase + n_returns) / den\n",
    "            self.n_of_purchase += self.collected_reward[time-30]\n",
    "\n",
    "        self.beta_parameteres[pulled_arm_price, 0] = self.beta_parameteres[pulled_arm_price, 0] + n_purchases\n",
    "        self.beta_parameteres[pulled_arm_price, 1] = self.beta_parameteres[pulled_arm_price, 1] + n_clicks - n_purchases\n",
    "\n",
    "        self.prob = prob\n",
    "        self.value_mean[pulled_arm_price] = (self.value_mean[pulled_arm_price]*self.time_pulled[pulled_arm_price]+daily_reward)/(self.time_pulled[pulled_arm_price]+1)\n",
    "        self.time_pulled[pulled_arm_price] += 1\n",
    "        # for each arm we update the confidence\n",
    "\n",
    "    def lowerbounds (self, t):\n",
    "        alpha = 0.05\n",
    "        time = t - self.bird\n",
    "        clicks = np.sum(self.n_plays)\n",
    "        low_value = self.value_mean[self.past_arms[time]] - np.sqrt(-np.log(alpha)/(2*(self.time_pulled[self.past_arms[time]]+1)))\n",
    "        low_prob = 1\n",
    "        if self.prob != 1:\n",
    "            low_prob = self.prob - np.sqrt(-np.log(alpha)/(2*clicks))\n",
    "        return low_value, low_prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
